## Day 1 â€” To-Do List

* [ ] Finish **Chapter 1** at [neuralnetworksanddeeplearning.com](http://neuralnetworksanddeeplearning.com)

  * [ ] Go through
  * [ ] Type out relevant code yourself and try

### ðŸ§  Math & Glossary

* [ ] Create a glossary of:

  * [ ] Math **symbols**
  * [ ] Unknown **terms**
  * [ ] Any unclear **PyTorch functions**

* [ ] Learn **PyTorch basics**

  * [ ] Tensors
  * [ ] Autograd
  * [ ] Training a simple model (e.g., MNIST)

### ðŸ“š Read 10 Key Deep Learning Papers from **2012**

1. [ ] AlexNet â€” *ImageNet Classification with Deep Convolutional Neural Networks* â€“ Krizhevsky et al.
2. [ ] Dropout â€” *Improving Neural Networks by Preventing Co-adaptation of Feature Detectors* â€“ Hinton et al.
3. [ ] Google Brain Cat Paper â€” *Building High-level Features Using Large Scale Unsupervised Learning* â€“ Le et al.
4. [ ] Multi-column DNNs â€” *Multi-column Deep Neural Networks for Image Classification* â€“ Ciresan et al.
5. [ ] Training Deep Architectures â€” *Practical Recommendations for Gradient-Based Training of Deep Architectures* â€“ Bengio
6. [ ] Acoustic Modeling â€” *Deep Neural Networks for Acoustic Modeling in Speech Recognition* â€“ Hinton et al.
7. [ ] SGD Tricks â€” *Stochastic Gradient Descent Tricks* â€“ Bottou
8. [ ] GPU-Based DNNs â€” *ImageNet Classification with Deep Convolutional Neural Networks* â€“ Krizhevsky et al.
9. [ ] Optimization Methods â€” *On Optimization Methods for Deep Learning* â€“ Le et al.
10. [ ] Representation Learning â€” *Representation Learning: A Review and New Perspectives* â€“ Bengio et al.

* [ ] Brush up stats basics:
  * [ ] Mean
  * [ ] Gaussian Distribution
  * [ ] Expectation
  * [ ] How **expectation = mean** for certain distributions

### ðŸ‘¥ Misc

* [ ] Make the **group email** for project.