## Day 1 â€” To-Do List

* [ ] Finish **Chapter 1** at [neuralnetworksanddeeplearning.com](http://neuralnetworksanddeeplearning.com)
* [ ] Learn **PyTorch basics**

  * [ ] Tensors
  * [ ] Autograd
  * [ ] Training a simple model (e.g., MNIST)

### ðŸ“š Read 10 Key Deep Learning Papers from **2012**

1. [ ] AlexNet â€” *ImageNet Classification with Deep Convolutional Neural Networks* â€“ Krizhevsky et al.
2. [ ] Deep Belief Nets â€” *A Practical Guide to Training Restricted Boltzmann Machines* â€“ Hinton (revisited in 2012)
3. [ ] Unsupervised Feature Learning â€” *Building High-level Features Using Large Scale Unsupervised Learning* â€“ Le et al. (Google Brain)
4. [ ] Dropout proposal â€” *Improving neural networks by preventing co-adaptation of feature detectors* â€“ Hinton et al. (early dropout version, 2012 arXiv)
5. [ ] RNN training improvements â€” *Learning to execute* (related works in 2012 on sequence learning)
6. [ ] Maxout networks precursor ideas
7. [ ] Deep sparse rectifier neural nets â€” *Rectified Linear Units Improve Restricted Boltzmann Machines* â€“ Nair & Hinton
8. [ ] *Learning representations by backpropagating errors* (classic rereleased/discussed)
9. [ ] *Stochastic Pooling for Regularization* â€“ Zeiler & Fergus
10. [ ] *Evolving deep architectures using genetic programming* â€“ Real et al.

### ðŸ§  Math & Glossary

* [ ] Create a glossary of:

  * [ ] Math **symbols**
  * [ ] Unknown **terms**
  * [ ] Any unclear **PyTorch functions**

* [ ] Brush up stats basics:
  * [ ] Mean
  * [ ] Gaussian Distribution
  * [ ] Expectation
  * [ ] How **expectation = mean** for certain distributions

### ðŸ‘¥ Misc

* [ ] Make the **group email** for project.